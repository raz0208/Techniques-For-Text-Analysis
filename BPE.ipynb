{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNVSWJpAgwXAv8m+zZKYOzK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raz0208/Techniques-For-Text-Analysis/blob/main/BPE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "GwWiolJ7aWbS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(vocab):\n",
        "    \"\"\"\n",
        "    Given a vocabulary (dictionary mapping words to frequency counts), returns a\n",
        "    dictionary of tuples representing the frequency count of pairs of characters\n",
        "    in the vocabulary.\n",
        "    \"\"\"\n",
        "    pairs = defaultdict(int)\n",
        "    for word, freq in vocab.items():\n",
        "        symbols = word.split()\n",
        "        for i in range(len(symbols)-1):\n",
        "            pairs[symbols[i],symbols[i+1]] += freq\n",
        "    return pairs"
      ],
      "metadata": {
        "id": "D5zZlo-bQtVN"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def merge_vocab(pair, v_in):\n",
        "    \"\"\"\n",
        "    Given a pair of characters and a vocabulary, returns a new vocabulary with the\n",
        "    pair of characters merged together wherever they appear.\n",
        "    \"\"\"\n",
        "    v_out = {}\n",
        "    bigram = re.escape(' '.join(pair))\n",
        "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
        "    for word in v_in:\n",
        "        w_out = p.sub(''.join(pair), word)\n",
        "        v_out[w_out] = v_in[word]\n",
        "    return v_out"
      ],
      "metadata": {
        "id": "ynDb8jOQae4u"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_vocab(data):\n",
        "    \"\"\"\n",
        "    Given a list of strings, returns a dictionary of words mapping to their frequency\n",
        "    count in the data.\n",
        "    \"\"\"\n",
        "    vocab = defaultdict(int)\n",
        "    for line in data:\n",
        "        for word in line.split():\n",
        "            vocab[' '.join(list(word)) + ' </w>'] += 1\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "i6Rlbfw6aiQ2"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def byte_pair_encoding(data, n):\n",
        "    \"\"\"\n",
        "    Given a list of strings and an integer n, returns a list of n merged pairs\n",
        "    of characters found in the vocabulary of the input data.\n",
        "    \"\"\"\n",
        "    vocab = get_vocab(data)\n",
        "    for i in range(n):\n",
        "        pairs = get_stats(vocab)\n",
        "        best = max(pairs, key=pairs.get)\n",
        "        vocab = merge_vocab(best, vocab)\n",
        "    return vocab"
      ],
      "metadata": {
        "id": "VJGtANz_alj_"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example usage:\n",
        "corpus = '''Tokenization is the process of breaking down\n",
        "a sequence of text into smaller units called tokens,\n",
        "which can be words, phrases, or even individual characters.\n",
        "Tokenization is often the first step in natural languages processing tasks\n",
        "such as text classification, named entity recognition, and sentiment analysis.\n",
        "The resulting tokens are typically used as input to further processing steps,\n",
        "such as vectorization, where the tokens are converted\n",
        "into numerical representations for machine learning models to use.'''\n",
        "data = corpus.split('.')\n",
        "\n",
        "n = 230\n",
        "bpe_pairs = byte_pair_encoding(data, n)\n",
        "bpe_pairs"
      ],
      "metadata": {
        "id": "DA1CsdeNanxf",
        "outputId": "b02cc869-7915-4ba2-df75-7704977e048d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'Tokenization</w>': 2,\n",
              " 'is</w>': 2,\n",
              " 'the</w>': 3,\n",
              " 'process</w>': 1,\n",
              " 'of</w>': 2,\n",
              " 'breaking</w>': 1,\n",
              " 'down</w>': 1,\n",
              " 'a</w>': 1,\n",
              " 'sequence</w>': 1,\n",
              " 'text</w>': 2,\n",
              " 'into</w>': 2,\n",
              " 'smaller</w>': 1,\n",
              " 'units</w>': 1,\n",
              " 'called</w>': 1,\n",
              " 'tokens,</w>': 1,\n",
              " 'which</w>': 1,\n",
              " 'can</w>': 1,\n",
              " 'be</w>': 1,\n",
              " 'words,</w>': 1,\n",
              " 'phrases,</w>': 1,\n",
              " 'or</w>': 1,\n",
              " 'even</w>': 1,\n",
              " 'individual</w>': 1,\n",
              " 'characters</w>': 1,\n",
              " 'often</w>': 1,\n",
              " 'first</w>': 1,\n",
              " 'step</w>': 1,\n",
              " 'in</w>': 1,\n",
              " 'natural</w>': 1,\n",
              " 'languages</w>': 1,\n",
              " 'processing</w>': 2,\n",
              " 'tasks</w>': 1,\n",
              " 'such</w>': 2,\n",
              " 'as</w>': 3,\n",
              " 'classification,</w>': 1,\n",
              " 'named</w>': 1,\n",
              " 'entity</w>': 1,\n",
              " 'recognition,</w>': 1,\n",
              " 'and</w>': 1,\n",
              " 'sentiment</w>': 1,\n",
              " 'analysis</w>': 1,\n",
              " 'The</w>': 1,\n",
              " 'resulting</w>': 1,\n",
              " 'tokens</w>': 2,\n",
              " 'are</w>': 2,\n",
              " 'typically</w>': 1,\n",
              " 'used</w>': 1,\n",
              " 'input</w>': 1,\n",
              " 'to</w>': 2,\n",
              " 'further</w>': 1,\n",
              " 'steps,</w>': 1,\n",
              " 'vectorization,</w>': 1,\n",
              " 'where</w>': 1,\n",
              " 'converted</w>': 1,\n",
              " 'numerical</w>': 1,\n",
              " 'representations</w>': 1,\n",
              " 'for</w>': 1,\n",
              " 'machine</w>': 1,\n",
              " 'learning</w>': 1,\n",
              " 'models</w>': 1,\n",
              " 'use</w>': 1}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    }
  ]
}