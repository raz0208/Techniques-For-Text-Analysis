{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsdxqKoUIzz6AqqrmbZTCd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raz0208/Techniques-For-Text-Analysis/blob/main/BagofWords(BoW).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bag of Words (BoW):\n",
        "BoW is a simple and widely used text representation technique in Natural Language Processing (NLP) and machine learning. It converts text data into numerical form, making it easier for models to process and analyze."
      ],
      "metadata": {
        "id": "Aq1f0IyXNLC3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## BoW Implementation code"
      ],
      "metadata": {
        "id": "u8h317cVNK0-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import libraries and read the data"
      ],
      "metadata": {
        "id": "ixLEfHoOSkym"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "5exmueErNKNK"
      },
      "outputs": [],
      "source": [
        "# Import laibraries\n",
        "import re\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the dataset\n",
        "documents = [\n",
        "    \"\"\"In the heart of the city, where the streets hum with the rhythm of daily life, a small café stood nestled\n",
        "    between towering buildings. The aroma of freshly brewed coffee mingled with the crisp morning air, drawing in\n",
        "    early risers and weary travelers alike. Inside, the café buzzed with quiet conversations, the clinking of cups,\n",
        "    and the occasional rustle of newspaper pages.\"\"\",\n",
        "\n",
        "    \"\"\"Among the patrons sat an old man, his eyes filled with the weight of years, gazing out the window as the world\n",
        "    passed by. Across from him, a young woman typed furiously on her laptop, her brow furrowed in concentration.\n",
        "    The barista, a cheerful fellow with a knack for remembering names, moved gracefully behind the counter,\n",
        "    crafting intricate patterns in the frothy tops of cappuccinos.\"\"\",\n",
        "\n",
        "    \"\"\"As the morning stretched into afternoon, the café remained a sanctuary—a temporary escape from the relentless\n",
        "    pace of the world outside. The city continued its symphony of honking horns and hurried footsteps, but within\n",
        "    these walls, time seemed to slow, allowing stories to unfold in whispered exchanges and silent reflections.\"\"\"\n",
        "]\n",
        "\n",
        "documents"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fjHkcTgYSX7A",
        "outputId": "ffbe3eaa-80b6-44bc-da21-774e3e2519f0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['In the heart of the city, where the streets hum with the rhythm of daily life, a small café stood nestled \\n    between towering buildings. The aroma of freshly brewed coffee mingled with the crisp morning air, drawing in \\n    early risers and weary travelers alike. Inside, the café buzzed with quiet conversations, the clinking of cups, \\n    and the occasional rustle of newspaper pages.',\n",
              " 'Among the patrons sat an old man, his eyes filled with the weight of years, gazing out the window as the world \\n    passed by. Across from him, a young woman typed furiously on her laptop, her brow furrowed in concentration. \\n    The barista, a cheerful fellow with a knack for remembering names, moved gracefully behind the counter, \\n    crafting intricate patterns in the frothy tops of cappuccinos.',\n",
              " 'As the morning stretched into afternoon, the café remained a sanctuary—a temporary escape from the relentless \\n    pace of the world outside. The city continued its symphony of honking horns and hurried footsteps, but within \\n    these walls, time seemed to slow, allowing stories to unfold in whispered exchanges and silent reflections.']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tokenization:\n",
        "- Split documents into words (tokens)."
      ],
      "metadata": {
        "id": "apXbgVIfUB-v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the corpus (by corpus_tokenizer function)\n",
        "def tokenizer(doc):\n",
        "    return [doc.lower().split() for doc in doc]\n",
        "\n",
        "tokenized_doc = tokenizer(documents)\n",
        "print(tokenized_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RnLiReu0UBwH",
        "outputId": "dd46f128-d1e5-496c-b11d-06bc652de18a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['in', 'the', 'heart', 'of', 'the', 'city,', 'where', 'the', 'streets', 'hum', 'with', 'the', 'rhythm', 'of', 'daily', 'life,', 'a', 'small', 'café', 'stood', 'nestled', 'between', 'towering', 'buildings.', 'the', 'aroma', 'of', 'freshly', 'brewed', 'coffee', 'mingled', 'with', 'the', 'crisp', 'morning', 'air,', 'drawing', 'in', 'early', 'risers', 'and', 'weary', 'travelers', 'alike.', 'inside,', 'the', 'café', 'buzzed', 'with', 'quiet', 'conversations,', 'the', 'clinking', 'of', 'cups,', 'and', 'the', 'occasional', 'rustle', 'of', 'newspaper', 'pages.'], ['among', 'the', 'patrons', 'sat', 'an', 'old', 'man,', 'his', 'eyes', 'filled', 'with', 'the', 'weight', 'of', 'years,', 'gazing', 'out', 'the', 'window', 'as', 'the', 'world', 'passed', 'by.', 'across', 'from', 'him,', 'a', 'young', 'woman', 'typed', 'furiously', 'on', 'her', 'laptop,', 'her', 'brow', 'furrowed', 'in', 'concentration.', 'the', 'barista,', 'a', 'cheerful', 'fellow', 'with', 'a', 'knack', 'for', 'remembering', 'names,', 'moved', 'gracefully', 'behind', 'the', 'counter,', 'crafting', 'intricate', 'patterns', 'in', 'the', 'frothy', 'tops', 'of', 'cappuccinos.'], ['as', 'the', 'morning', 'stretched', 'into', 'afternoon,', 'the', 'café', 'remained', 'a', 'sanctuary—a', 'temporary', 'escape', 'from', 'the', 'relentless', 'pace', 'of', 'the', 'world', 'outside.', 'the', 'city', 'continued', 'its', 'symphony', 'of', 'honking', 'horns', 'and', 'hurried', 'footsteps,', 'but', 'within', 'these', 'walls,', 'time', 'seemed', 'to', 'slow,', 'allowing', 'stories', 'to', 'unfold', 'in', 'whispered', 'exchanges', 'and', 'silent', 'reflections.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build Vocabulary:\n",
        "- Create a unique set of words from all documents."
      ],
      "metadata": {
        "id": "g6sn5ahRWVR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a vocabulary (by build_vocab function)\n",
        "def build_vocab(doc):\n",
        "  vocab = set()\n",
        "  for tokens in doc:\n",
        "    vocab.update(tokens)\n",
        "  return sorted(vocab)\n",
        "\n",
        "vocab = build_vocab(tokenized_doc)\n",
        "print(vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SwN9jMDaVaEg",
        "outputId": "2ca6e859-6916-4d02-99dc-20a2612e5e19"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'across', 'afternoon,', 'air,', 'alike.', 'allowing', 'among', 'an', 'and', 'aroma', 'as', 'barista,', 'behind', 'between', 'brewed', 'brow', 'buildings.', 'but', 'buzzed', 'by.', 'café', 'cappuccinos.', 'cheerful', 'city', 'city,', 'clinking', 'coffee', 'concentration.', 'continued', 'conversations,', 'counter,', 'crafting', 'crisp', 'cups,', 'daily', 'drawing', 'early', 'escape', 'exchanges', 'eyes', 'fellow', 'filled', 'footsteps,', 'for', 'freshly', 'from', 'frothy', 'furiously', 'furrowed', 'gazing', 'gracefully', 'heart', 'her', 'him,', 'his', 'honking', 'horns', 'hum', 'hurried', 'in', 'inside,', 'into', 'intricate', 'its', 'knack', 'laptop,', 'life,', 'man,', 'mingled', 'morning', 'moved', 'names,', 'nestled', 'newspaper', 'occasional', 'of', 'old', 'on', 'out', 'outside.', 'pace', 'pages.', 'passed', 'patrons', 'patterns', 'quiet', 'reflections.', 'relentless', 'remained', 'remembering', 'rhythm', 'risers', 'rustle', 'sanctuary—a', 'sat', 'seemed', 'silent', 'slow,', 'small', 'stood', 'stories', 'streets', 'stretched', 'symphony', 'temporary', 'the', 'these', 'time', 'to', 'tops', 'towering', 'travelers', 'typed', 'unfold', 'walls,', 'weary', 'weight', 'where', 'whispered', 'window', 'with', 'within', 'woman', 'world', 'years,', 'young']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Vectorization\n",
        "- Convert each document into a vector:\n",
        "  *   Count-based: Number of times each word appears.\n",
        "  *   Binary-based: Presence (1) or absence (0) of words.\n",
        "\n"
      ],
      "metadata": {
        "id": "IMltSA6mWy5v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorization: Create BoW matrix\n",
        "def vectorize_docs(tokenized_docs, vocab, mode=\"count\"):\n",
        "    \"\"\"\n",
        "    mode: \"count\" for count-based, \"binary\" for binary-based representation.\n",
        "    \"\"\"\n",
        "    num_docs = len(tokenized_docs)\n",
        "    vocab_size = len(vocab)\n",
        "    # Initialize matrix with zeros\n",
        "    bow_matrix = np.zeros((num_docs, vocab_size), dtype=int)\n",
        "\n",
        "    # Map each word in vocabulary to its index\n",
        "    vocab_index = {word: idx for idx, word in enumerate(vocab)}\n",
        "\n",
        "    for i, tokens in enumerate(tokenized_docs):\n",
        "        for token in tokens:\n",
        "            idx = vocab_index[token]\n",
        "            if mode == \"count\":\n",
        "                bow_matrix[i, idx] += 1\n",
        "            elif mode == \"binary\":\n",
        "                bow_matrix[i, idx] = 1\n",
        "    return bow_matrix\n",
        "\n",
        "# Create count-based BoW matrix\n",
        "bow_count = vectorize_docs(tokenized_doc, vocab, mode=\"count\")\n",
        "print(\"\\nCount-based BoW Matrix:\")\n",
        "print(bow_count)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cmOEZfCZW9j3",
        "outputId": "6cedbf2b-92f8-46a0-fa82-d0de69fb3464"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Count-based BoW Matrix:\n",
            "[[1 0 0 1 1 0 0 0 2 1 0 0 0 1 1 0 1 0 1 0 2 0 0 0 1 1 1 0 0 1 0 0 1 1 1 1\n",
            "  1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 2 1 0 0 0 0 0 1 0 1 1 0 0\n",
            "  1 1 1 5 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 9 0 0\n",
            "  0 0 1 1 0 0 0 1 0 1 0 0 3 0 0 0 0 0]\n",
            " [3 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0\n",
            "  0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 2 1 1 0 0 0 0 2 0 0 1 0 1 1 0 1 0 0 1 1\n",
            "  0 0 0 2 1 1 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 7 0 0\n",
            "  0 1 0 0 1 0 0 0 1 0 0 1 2 0 1 1 1 1]\n",
            " [1 0 1 0 0 1 0 0 2 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            "  0 0 0 2 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 5 1 1\n",
            "  2 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create binary-based BoW matrix\n",
        "bow_binary = vectorize_docs(tokenized_doc, vocab, mode=\"binary\")\n",
        "print(\"\\nBinary-based BoW Matrix:\")\n",
        "print(bow_binary)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MUEclgu_Yy5o",
        "outputId": "df8f1bc8-69c1-487a-8d86-4f6adabd7917"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Binary-based BoW Matrix:\n",
            "[[1 0 0 1 1 0 0 0 1 1 0 0 0 1 1 0 1 0 1 0 1 0 0 0 1 1 1 0 0 1 0 0 1 1 1 1\n",
            "  1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 0 0 1 0 1 1 0 0 0 0 0 1 0 1 1 0 0\n",
            "  1 1 1 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 1 0 0 0 0 0 1 1 0 1 0 0 0 1 0 0\n",
            "  0 0 1 1 0 0 0 1 0 1 0 0 1 0 0 0 0 0]\n",
            " [1 1 0 0 0 0 1 1 0 0 1 1 1 0 0 1 0 0 0 1 0 1 1 0 0 0 0 1 0 0 1 1 0 0 0 0\n",
            "  0 0 0 1 1 1 0 1 0 1 1 1 1 1 1 0 1 1 1 0 0 0 0 1 0 0 1 0 1 1 0 1 0 0 1 1\n",
            "  0 0 0 1 1 1 1 0 0 0 1 1 1 0 0 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
            "  0 1 0 0 1 0 0 0 1 0 0 1 1 0 1 1 1 1]\n",
            " [1 0 1 0 0 1 0 0 1 0 1 0 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 1 0 0 0 0 0 0 0\n",
            "  0 1 1 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 1 0 1 0 0 0 0 0 1 0 0\n",
            "  0 0 0 1 0 0 0 1 1 0 0 0 0 0 1 1 1 0 0 0 0 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1\n",
            "  1 0 0 0 0 1 1 0 0 0 1 0 0 1 0 1 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization function to normalize each document vector\n",
        "def normalize_bow_matrix(bow_matrix, norm='l1'):\n",
        "    \"\"\"\n",
        "    Normalizes the BoW matrix using L1 or L2 normalization.\n",
        "\n",
        "    Parameters:\n",
        "      bow_matrix : numpy.ndarray\n",
        "          The BoW matrix.\n",
        "      norm : str, optional\n",
        "          Type of normalization ('l1' or 'l2'). Default is 'l1'.\n",
        "\n",
        "    Returns:\n",
        "      normalized_matrix : numpy.ndarray\n",
        "          The normalized BoW matrix.\n",
        "    \"\"\"\n",
        "    if norm == 'l1':\n",
        "        # L1 normalization: divide each row by its sum\n",
        "        row_sums = bow_matrix.sum(axis=1, keepdims=True)\n",
        "        normalized_matrix = bow_matrix / np.where(row_sums == 0, 1, row_sums)\n",
        "    elif norm == 'l2':\n",
        "        # L2 normalization: divide each row by its Euclidean norm\n",
        "        row_norms = np.linalg.norm(bow_matrix, axis=1, keepdims=True)\n",
        "        normalized_matrix = bow_matrix / np.where(row_norms == 0, 1, row_norms)\n",
        "    else:\n",
        "        raise ValueError(\"Unsupported normalization type. Choose 'l1' or 'l2'.\")\n",
        "    return normalized_matrix\n",
        "\n",
        "# Normalize the count-based BoW matrix (using L1 normalization)\n",
        "normalized_bow = normalize_bow_matrix(bow_count, norm='l1')\n",
        "print(\"\\nNormalized (L1) Count-based BoW Matrix:\")\n",
        "print(normalized_bow)\n",
        "\n",
        "# Optionally, you can wrap the entire BoW pipeline into one function that returns the normalized BoW matrix and vocabulary\n",
        "def bag_of_words_pipeline(documents, mode='count', norm='l1'):\n",
        "    # Tokenize documents\n",
        "    tokenized_docs = tokenizer(documents)\n",
        "    # Build vocabulary\n",
        "    vocab = build_vocab(tokenized_docs)\n",
        "    # Vectorize documents\n",
        "    bow_matrix = vectorize_docs(tokenized_docs, vocab, mode=mode)\n",
        "    # Normalize the BoW matrix\n",
        "    normalized_matrix = normalize_bow_matrix(bow_matrix, norm=norm)\n",
        "    return normalized_matrix, vocab\n",
        "\n",
        "# Get final normalized BoW matrix and vocabulary\n",
        "final_normalized_bow, final_vocab = bag_of_words_pipeline(documents, mode=\"count\", norm=\"l1\")\n",
        "print(\"\\nFinal Normalized BoW Matrix and Vocabulary:\")\n",
        "print(final_normalized_bow)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5du7j-0aCPn",
        "outputId": "e4b1f035-5109-4d2a-adf6-87e45ed65809"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Normalized (L1) Count-based BoW Matrix:\n",
            "[[0.01612903 0.         0.         0.01612903 0.01612903 0.\n",
            "  0.         0.         0.03225806 0.01612903 0.         0.\n",
            "  0.         0.01612903 0.01612903 0.         0.01612903 0.\n",
            "  0.01612903 0.         0.03225806 0.         0.         0.\n",
            "  0.01612903 0.01612903 0.01612903 0.         0.         0.01612903\n",
            "  0.         0.         0.01612903 0.01612903 0.01612903 0.01612903\n",
            "  0.01612903 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.01612903 0.         0.         0.\n",
            "  0.         0.         0.         0.01612903 0.         0.\n",
            "  0.         0.         0.         0.01612903 0.         0.03225806\n",
            "  0.01612903 0.         0.         0.         0.         0.\n",
            "  0.01612903 0.         0.01612903 0.01612903 0.         0.\n",
            "  0.01612903 0.01612903 0.01612903 0.08064516 0.         0.\n",
            "  0.         0.         0.         0.01612903 0.         0.\n",
            "  0.         0.01612903 0.         0.         0.         0.\n",
            "  0.01612903 0.01612903 0.01612903 0.         0.         0.\n",
            "  0.         0.         0.01612903 0.01612903 0.         0.01612903\n",
            "  0.         0.         0.         0.14516129 0.         0.\n",
            "  0.         0.         0.01612903 0.01612903 0.         0.\n",
            "  0.         0.01612903 0.         0.01612903 0.         0.\n",
            "  0.0483871  0.         0.         0.         0.         0.        ]\n",
            " [0.04615385 0.01538462 0.         0.         0.         0.\n",
            "  0.01538462 0.01538462 0.         0.         0.01538462 0.01538462\n",
            "  0.01538462 0.         0.         0.01538462 0.         0.\n",
            "  0.         0.01538462 0.         0.01538462 0.01538462 0.\n",
            "  0.         0.         0.         0.01538462 0.         0.\n",
            "  0.01538462 0.01538462 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.01538462 0.01538462 0.01538462\n",
            "  0.         0.01538462 0.         0.01538462 0.01538462 0.01538462\n",
            "  0.01538462 0.01538462 0.01538462 0.         0.03076923 0.01538462\n",
            "  0.01538462 0.         0.         0.         0.         0.03076923\n",
            "  0.         0.         0.01538462 0.         0.01538462 0.01538462\n",
            "  0.         0.01538462 0.         0.         0.01538462 0.01538462\n",
            "  0.         0.         0.         0.03076923 0.01538462 0.01538462\n",
            "  0.01538462 0.         0.         0.         0.01538462 0.01538462\n",
            "  0.01538462 0.         0.         0.         0.         0.01538462\n",
            "  0.         0.         0.         0.         0.01538462 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.10769231 0.         0.\n",
            "  0.         0.01538462 0.         0.         0.01538462 0.\n",
            "  0.         0.         0.01538462 0.         0.         0.01538462\n",
            "  0.03076923 0.         0.01538462 0.01538462 0.01538462 0.01538462]\n",
            " [0.02       0.         0.02       0.         0.         0.02\n",
            "  0.         0.         0.04       0.         0.02       0.\n",
            "  0.         0.         0.         0.         0.         0.02\n",
            "  0.         0.         0.02       0.         0.         0.02\n",
            "  0.         0.         0.         0.         0.02       0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.02       0.02       0.         0.         0.\n",
            "  0.02       0.         0.         0.02       0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.02       0.02       0.         0.02       0.02\n",
            "  0.         0.02       0.         0.02       0.         0.\n",
            "  0.         0.         0.         0.02       0.         0.\n",
            "  0.         0.         0.         0.04       0.         0.\n",
            "  0.         0.02       0.02       0.         0.         0.\n",
            "  0.         0.         0.02       0.02       0.02       0.\n",
            "  0.         0.         0.         0.02       0.         0.02\n",
            "  0.02       0.02       0.         0.         0.02       0.\n",
            "  0.02       0.02       0.02       0.1        0.02       0.02\n",
            "  0.04       0.         0.         0.         0.         0.02\n",
            "  0.02       0.         0.         0.         0.02       0.\n",
            "  0.         0.02       0.         0.02       0.         0.        ]]\n",
            "\n",
            "Final Normalized BoW Matrix and Vocabulary:\n",
            "[[0.01612903 0.         0.         0.01612903 0.01612903 0.\n",
            "  0.         0.         0.03225806 0.01612903 0.         0.\n",
            "  0.         0.01612903 0.01612903 0.         0.01612903 0.\n",
            "  0.01612903 0.         0.03225806 0.         0.         0.\n",
            "  0.01612903 0.01612903 0.01612903 0.         0.         0.01612903\n",
            "  0.         0.         0.01612903 0.01612903 0.01612903 0.01612903\n",
            "  0.01612903 0.         0.         0.         0.         0.\n",
            "  0.         0.         0.01612903 0.         0.         0.\n",
            "  0.         0.         0.         0.01612903 0.         0.\n",
            "  0.         0.         0.         0.01612903 0.         0.03225806\n",
            "  0.01612903 0.         0.         0.         0.         0.\n",
            "  0.01612903 0.         0.01612903 0.01612903 0.         0.\n",
            "  0.01612903 0.01612903 0.01612903 0.08064516 0.         0.\n",
            "  0.         0.         0.         0.01612903 0.         0.\n",
            "  0.         0.01612903 0.         0.         0.         0.\n",
            "  0.01612903 0.01612903 0.01612903 0.         0.         0.\n",
            "  0.         0.         0.01612903 0.01612903 0.         0.01612903\n",
            "  0.         0.         0.         0.14516129 0.         0.\n",
            "  0.         0.         0.01612903 0.01612903 0.         0.\n",
            "  0.         0.01612903 0.         0.01612903 0.         0.\n",
            "  0.0483871  0.         0.         0.         0.         0.        ]\n",
            " [0.04615385 0.01538462 0.         0.         0.         0.\n",
            "  0.01538462 0.01538462 0.         0.         0.01538462 0.01538462\n",
            "  0.01538462 0.         0.         0.01538462 0.         0.\n",
            "  0.         0.01538462 0.         0.01538462 0.01538462 0.\n",
            "  0.         0.         0.         0.01538462 0.         0.\n",
            "  0.01538462 0.01538462 0.         0.         0.         0.\n",
            "  0.         0.         0.         0.01538462 0.01538462 0.01538462\n",
            "  0.         0.01538462 0.         0.01538462 0.01538462 0.01538462\n",
            "  0.01538462 0.01538462 0.01538462 0.         0.03076923 0.01538462\n",
            "  0.01538462 0.         0.         0.         0.         0.03076923\n",
            "  0.         0.         0.01538462 0.         0.01538462 0.01538462\n",
            "  0.         0.01538462 0.         0.         0.01538462 0.01538462\n",
            "  0.         0.         0.         0.03076923 0.01538462 0.01538462\n",
            "  0.01538462 0.         0.         0.         0.01538462 0.01538462\n",
            "  0.01538462 0.         0.         0.         0.         0.01538462\n",
            "  0.         0.         0.         0.         0.01538462 0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.10769231 0.         0.\n",
            "  0.         0.01538462 0.         0.         0.01538462 0.\n",
            "  0.         0.         0.01538462 0.         0.         0.01538462\n",
            "  0.03076923 0.         0.01538462 0.01538462 0.01538462 0.01538462]\n",
            " [0.02       0.         0.02       0.         0.         0.02\n",
            "  0.         0.         0.04       0.         0.02       0.\n",
            "  0.         0.         0.         0.         0.         0.02\n",
            "  0.         0.         0.02       0.         0.         0.02\n",
            "  0.         0.         0.         0.         0.02       0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.02       0.02       0.         0.         0.\n",
            "  0.02       0.         0.         0.02       0.         0.\n",
            "  0.         0.         0.         0.         0.         0.\n",
            "  0.         0.02       0.02       0.         0.02       0.02\n",
            "  0.         0.02       0.         0.02       0.         0.\n",
            "  0.         0.         0.         0.02       0.         0.\n",
            "  0.         0.         0.         0.04       0.         0.\n",
            "  0.         0.02       0.02       0.         0.         0.\n",
            "  0.         0.         0.02       0.02       0.02       0.\n",
            "  0.         0.         0.         0.02       0.         0.02\n",
            "  0.02       0.02       0.         0.         0.02       0.\n",
            "  0.02       0.02       0.02       0.1        0.02       0.02\n",
            "  0.04       0.         0.         0.         0.         0.02\n",
            "  0.02       0.         0.         0.         0.02       0.\n",
            "  0.         0.02       0.         0.02       0.         0.        ]]\n"
          ]
        }
      ]
    }
  ]
}