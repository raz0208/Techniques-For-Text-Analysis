{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMDr7+u6ZU4vHPqtgULbv3D",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raz0208/Techniques-For-Text-Analysis/blob/main/Lemmatization_Stemming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization\n",
        "Lemmatization is a natural language processing (NLP) technique that reduces words to their base or dictionary form, known as a lemma. Unlike simple stemming—which might just chop off word endings—lemmatization uses linguistic rules and context (like a word’s part of speech) to ensure that the transformed word is meaningful.\n",
        "\n",
        "For example, \"running,\" \"ran,\" and \"runs\" are all reduced to the lemma \"run.\" This process is essential for tasks like text analysis and search because it helps consolidate different forms of a word into one common representation."
      ],
      "metadata": {
        "id": "T90TU5fNgPXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import libraries and read data"
      ],
      "metadata": {
        "id": "Wp62pk5FhfiY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pgzAQ8JjgMQa"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "E0_MA3ulSQ7V",
        "outputId": "eb96d288-8f76-4e91-e19c-6876df9b9def"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nltk\n",
            "  Downloading nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk) (4.67.1)\n",
            "Downloading nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nltk\n",
            "Successfully installed nltk-3.9.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "nltk"
                ]
              },
              "id": "4931e27b93154a0689ee821f74b14c60"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data files (only needed once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dEoT5xNhx9_",
        "outputId": "2795d5af-4123-4989-e495-80700d25956c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Text Data (Replace with your text source if needed)\n",
        "document = (\n",
        "    \"This is an example sentence, demonstrating the process of lemmatization. \"\n",
        "    \"Running, ran, and runs are different forms of the verb run.\"\n",
        ")\n",
        "\n",
        "document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SGs1a43oh1t3",
        "outputId": "30e5a6fe-ae0a-4a88-cb95-c718c39002fe"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is an example sentence, demonstrating the process of lemmatization. Running, ran, and runs are different forms of the verb run.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Tokenization"
      ],
      "metadata": {
        "id": "SsJbe3_xiK84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokens = word_tokenize(document)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ9z4JJJh_z8",
        "outputId": "e78da5b1-9e7b-4735-9100-128b88027ef0"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'an', 'example', 'sentence', ',', 'demonstrating', 'the', 'process', 'of', 'lemmatization', '.', 'Running', ',', 'ran', ',', 'and', 'runs', 'are', 'different', 'forms', 'of', 'the', 'verb', 'run', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [token.lower() for token in tokens]\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oYVRuk8mYhi",
        "outputId": "2aab05c4-c536-4301-e328-52b6b79ef4aa"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'is', 'an', 'example', 'sentence', ',', 'demonstrating', 'the', 'process', 'of', 'lemmatization', '.', 'running', ',', 'ran', ',', 'and', 'runs', 'are', 'different', 'forms', 'of', 'the', 'verb', 'run', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Remove Stopwords & Punctuation"
      ],
      "metadata": {
        "id": "cdGZMkLgipc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords & Punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [token for token in tokens if token.lower() not in stop_words and token not in string.punctuation]\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fg6ISK2ij2f",
        "outputId": "ea6dd85c-222e-4a5b-9afb-d07d5bb5017d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['example', 'sentence', 'demonstrating', 'process', 'lemmatization', 'running', 'ran', 'runs', 'different', 'forms', 'verb', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Lemmatization using nltk with correct POS tags"
      ],
      "metadata": {
        "id": "ug3MfzcyjbV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to map NLTK POS tags to WordNet POS tags\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "# Lemmatization using nltk with correct POS tags\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# First, get POS tags for the tokens\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "# Then lemmatize each token with its corresponding POS tag\n",
        "lemmatized_tokens = [\n",
        "    lemmatizer.lemmatize(token, get_wordnet_pos(pos))\n",
        "    for token, pos in pos_tags\n",
        "]\n",
        "\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXQwXyXsjTnX",
        "outputId": "c825e043-5ba3-409d-cc09-efc317050a91"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['example', 'sentence', 'demonstrate', 'process', 'lemmatization', 'run', 'ran', 'run', 'different', 'form', 'verb', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Reconstruct Processed Text"
      ],
      "metadata": {
        "id": "zJJ7KdbRkFZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconstruct Processed Text\n",
        "processed_text_nltk = \" \".join(lemmatized_tokens)\n",
        "print(\"Processed Text (nltk):\")\n",
        "print(processed_text_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP7C4b-WkDIf",
        "outputId": "28f191dc-a620-4c8c-eaa7-96b45dac6270"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Text (nltk):\n",
            "example sentence demonstrate process lemmatization run ran run different form verb run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Save the Processed Data to a File"
      ],
      "metadata": {
        "id": "i9nFDxeRkXXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Processed Data to a File\n",
        "with open(\"processed_text_nltk.txt\", \"w\") as f:\n",
        "    f.write(processed_text_nltk)"
      ],
      "metadata": {
        "id": "2ynXIzxbkTEn"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Using spaCy for Text Processing\n",
        "# ------------------------------\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load spaCy's English language model\n",
        "# Make sure to run: python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(document)\n",
        "\n",
        "# 3 & 4. Remove stopwords & punctuation, then lemmatize using spaCy's attributes\n",
        "lemmatized_tokens_spacy = [\n",
        "    token.lemma_.lower() for token in doc if not token.is_stop and not token.is_punct\n",
        "]\n",
        "\n",
        "# Step 7: Dependency parsing\n",
        "def dependency_parsing(text):\n",
        "    doc = nlp(text)\n",
        "    return [(token.text, token.dep_, token.head.text) for token in doc]\n",
        "\n",
        "dep_test = dependency_parsing(document)\n",
        "print(dep_test)\n",
        "\n",
        "# 5. Reconstruct Processed Text\n",
        "processed_text_spacy = \" \".join(lemmatized_tokens_spacy)\n",
        "print(\"\\nProcessed Text (spaCy):\")\n",
        "print(processed_text_spacy)\n",
        "\n",
        "# 6. Save the Processed Data to a File\n",
        "with open(\"processed_text_spacy.txt\", \"w\") as f:\n",
        "    f.write(processed_text_spacy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P22LWSZLkoyQ",
        "outputId": "e4e32bbf-0276-4c11-e73e-1bedbc37a5bc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('This', 'nsubj', 'is'), ('is', 'ROOT', 'is'), ('an', 'det', 'sentence'), ('example', 'compound', 'sentence'), ('sentence', 'attr', 'is'), (',', 'punct', 'is'), ('demonstrating', 'advcl', 'is'), ('the', 'det', 'process'), ('process', 'dobj', 'demonstrating'), ('of', 'prep', 'process'), ('lemmatization', 'pobj', 'of'), ('.', 'punct', 'is'), ('Running', 'nsubj', 'ran'), (',', 'punct', 'Running'), ('ran', 'ROOT', 'ran'), (',', 'punct', 'ran'), ('and', 'cc', 'ran'), ('runs', 'nsubj', 'are'), ('are', 'conj', 'ran'), ('different', 'amod', 'forms'), ('forms', 'attr', 'are'), ('of', 'prep', 'forms'), ('the', 'det', 'run'), ('verb', 'compound', 'run'), ('run', 'pobj', 'of'), ('.', 'punct', 'are')]\n",
            "\n",
            "Processed Text (spaCy):\n",
            "example sentence demonstrate process lemmatization running run run different form verb run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming\n",
        "Stemming is a natural language processing (NLP) technique where words are reduced to their root or base form by cutting off prefixes or suffixes. The goal is to treat different variations of a word as the same for text analysis. For example:\n",
        "\n",
        "    \"running\" → \"run\"\n",
        "    \"happily\" → \"happi\"\n",
        "    \"better\" → \"better\" (some stemmers may not handle irregular forms well)\n",
        "\n",
        "Stemming often uses simple, rule-based approaches and can sometimes produce non-meaningful roots because it just chops off parts of the word without understanding the language.\n",
        "\n",
        "A more advanced technique, Lemmatization, reduces words to their dictionary (or lemma) form with context — like converting \"better\" to \"good\"."
      ],
      "metadata": {
        "id": "jf-9FBTip7He"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import libraries and read data"
      ],
      "metadata": {
        "id": "MDzRupUwqbyg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
        "import string"
      ],
      "metadata": {
        "id": "PTrh92BUp6E8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LGsW2OCbqkVo",
        "outputId": "22e82466-f07b-4994-ebc3-d9207e3e2b59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read data\n",
        "text = \"This is a simple example to demonstrate stemming. Stemming reduces words to their root form, which helps in text processing tasks.\"\n",
        "\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qmO8m1Miqodv",
        "outputId": "386c7e84-94ab-4a6b-9561-a996e7bd39d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is a simple example to demonstrate stemming. Stemming reduces words to their root form, which helps in text processing tasks.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Tokenization"
      ],
      "metadata": {
        "id": "XxgeXzWKqyx3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokens = word_tokenize(text.lower())\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPOcmapMqw6f",
        "outputId": "e47722ad-1702-4698-8535-fa338d24b9c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['this', 'is', 'a', 'simple', 'example', 'to', 'demonstrate', 'stemming', '.', 'stemming', 'reduces', 'words', 'to', 'their', 'root', 'form', ',', 'which', 'helps', 'in', 'text', 'processing', 'tasks', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Remove stopwords and punctuation"
      ],
      "metadata": {
        "id": "kDzCKZmorABP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stopwords and punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_tokens = [word for word in tokens if word not in stop_words and word not in string.punctuation]\n",
        "\n",
        "print(filtered_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PanlHSoNq84Q",
        "outputId": "22ba6b4b-bd12-4b7f-c299-c1bb3d35de8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['simple', 'example', 'demonstrate', 'stemming', 'stemming', 'reduces', 'words', 'root', 'form', 'helps', 'text', 'processing', 'tasks']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Apply Stemming\n",
        "- Use a stemming algorithm to reduce words to their base form:\n",
        "  - Porter Stemmer (nltk.PorterStemmer())\n",
        "  - Lancaster Stemmer (nltk.LancasterStemmer())\n",
        "  - Snowball Stemmer (nltk.SnowballStemmer())\n",
        "- Process tokens by applying the stemmer."
      ],
      "metadata": {
        "id": "rjCekNrErdjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply Stemming\n",
        "porter = PorterStemmer()\n",
        "lancaster = LancasterStemmer()\n",
        "snowball = SnowballStemmer('english')\n",
        "\n",
        "# Process tokens with each stemmer\n",
        "porter_stems = [porter.stem(word) for word in filtered_tokens]\n",
        "lancaster_stems = [lancaster.stem(word) for word in filtered_tokens]\n",
        "snowball_stems = [snowball.stem(word) for word in filtered_tokens]\n",
        "\n",
        "print(porter_stems, \"\\n\")\n",
        "print(lancaster_stems, \"\\n\")\n",
        "print(snowball_stems)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s1LWRl7ora5_",
        "outputId": "e6d79743-afe0-47bf-cd97-00d43f90a6af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['simpl', 'exampl', 'demonstr', 'stem', 'stem', 'reduc', 'word', 'root', 'form', 'help', 'text', 'process', 'task'] \n",
            "\n",
            "['simpl', 'exampl', 'demonst', 'stem', 'stem', 'reduc', 'word', 'root', 'form', 'help', 'text', 'process', 'task'] \n",
            "\n",
            "['simpl', 'exampl', 'demonstr', 'stem', 'stem', 'reduc', 'word', 'root', 'form', 'help', 'text', 'process', 'task']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Reconstruct processed text\n",
        "- Rebuild text from stemmed tokens."
      ],
      "metadata": {
        "id": "2FVADwI_sLEP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconstruct processed text\n",
        "porter_text = ' '.join(porter_stems)\n",
        "lancaster_text = ' '.join(lancaster_stems)\n",
        "snowball_text = ' '.join(snowball_stems)\n",
        "\n",
        "print(\"Porter Stemmer Text:\", porter_text, \"\\n\")\n",
        "print(\"Lancaster Stemmer Text:\", lancaster_text, \"\\n\")\n",
        "print(\"Snowball Stemmer Text:\", snowball_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jnpFMYBMsIxn",
        "outputId": "8f1a92f1-a128-4e84-801c-f485675d8e75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Porter Stemmer Text: simpl exampl demonstr stem stem reduc word root form help text process task \n",
            "\n",
            "Lancaster Stemmer Text: simpl exampl demonst stem stem reduc word root form help text process task \n",
            "\n",
            "Snowball Stemmer Text: simpl exampl demonstr stem stem reduc word root form help text process task\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Save or use the processed data"
      ],
      "metadata": {
        "id": "oyXJ5NXPszon"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save or use the processed data\n",
        "with open('processed_text_porter.txt', 'w') as f:\n",
        "    f.write(porter_text)\n",
        "\n",
        "with open('processed_text_lancaster.txt', 'w') as f:\n",
        "    f.write(lancaster_text)\n",
        "\n",
        "with open('processed_text_snowball.txt', 'w') as f:\n",
        "    f.write(snowball_text)\n",
        "\n",
        "print(\"Processed text saved successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kx2BZVYdswjP",
        "outputId": "63f51936-ce22-4307-e6a6-d2be4aa8075e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed text saved successfully!\n"
          ]
        }
      ]
    }
  ]
}