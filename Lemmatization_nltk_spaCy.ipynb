{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOi/P8mdFWky3fLkV1oB58F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raz0208/Techniques-For-Text-Analysis/blob/main/Lemmatization_nltk_spaCy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization\n",
        "Lemmatization is a natural language processing (NLP) technique that reduces words to their base or dictionary form, known as a lemma. Unlike simple stemming—which might just chop off word endings—lemmatization uses linguistic rules and context (like a word’s part of speech) to ensure that the transformed word is meaningful.\n",
        "\n",
        "For example, \"running,\" \"ran,\" and \"runs\" are all reduced to the lemma \"run.\" This process is essential for tasks like text analysis and search because it helps consolidate different forms of a word into one common representation."
      ],
      "metadata": {
        "id": "T90TU5fNgPXQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import libraries and read data"
      ],
      "metadata": {
        "id": "Wp62pk5FhfiY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "pgzAQ8JjgMQa"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "import nltk\n",
        "import string\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download necessary NLTK data files (only needed once)\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger_eng')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2dEoT5xNhx9_",
        "outputId": "ac9c14a2-0453-4238-c6cc-65f65aced6ad"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger_eng.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Text Data (Replace with your text source if needed)\n",
        "document = (\n",
        "    \"This is an example sentence, demonstrating the process of lemmatization. \"\n",
        "    \"Running, ran, and runs are different forms of the verb run.\"\n",
        ")\n",
        "\n",
        "document"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "SGs1a43oh1t3",
        "outputId": "6690b188-8335-4d94-88bc-e5cfbbe0c6ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'This is an example sentence, demonstrating the process of lemmatization. Running, ran, and runs are different forms of the verb run.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Tokenization"
      ],
      "metadata": {
        "id": "SsJbe3_xiK84"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "tokens = word_tokenize(document)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ9z4JJJh_z8",
        "outputId": "6172094d-a879-487d-f835-c2dfdb0eccdf"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['This', 'is', 'an', 'example', 'sentence', ',', 'demonstrating', 'the', 'process', 'of', 'lemmatization', '.', 'Running', ',', 'ran', ',', 'and', 'runs', 'are', 'different', 'forms', 'of', 'the', 'verb', 'run', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = [token.lower() for token in tokens]\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9oYVRuk8mYhi",
        "outputId": "cd40060d-b2e5-416b-ee42-beea0a6ec0c5"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['example', 'sentence', 'demonstrating', 'process', 'lemmatization', 'running', 'ran', 'runs', 'different', 'forms', 'verb', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Remove Stopwords & Punctuation"
      ],
      "metadata": {
        "id": "cdGZMkLgipc_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords & Punctuation\n",
        "stop_words = set(stopwords.words('english'))\n",
        "tokens = [token for token in tokens if token.lower() not in stop_words and token not in string.punctuation]\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fg6ISK2ij2f",
        "outputId": "ee95c4d2-a19f-450a-854b-a450f22c18a4"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['example', 'sentence', 'demonstrating', 'process', 'lemmatization', 'running', 'ran', 'runs', 'different', 'forms', 'verb', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Lemmatization using nltk with correct POS tags"
      ],
      "metadata": {
        "id": "ug3MfzcyjbV_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper function to map NLTK POS tags to WordNet POS tags\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "# Lemmatization using nltk with correct POS tags\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "# First, get POS tags for the tokens\n",
        "pos_tags = nltk.pos_tag(tokens)\n",
        "# Then lemmatize each token with its corresponding POS tag\n",
        "lemmatized_tokens = [\n",
        "    lemmatizer.lemmatize(token, get_wordnet_pos(pos))\n",
        "    for token, pos in pos_tags\n",
        "]\n",
        "\n",
        "print(lemmatized_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qXQwXyXsjTnX",
        "outputId": "37d97abe-33b3-4f00-def6-10af71314cc1"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['example', 'sentence', 'demonstrate', 'process', 'lemmatization', 'run', 'ran', 'run', 'different', 'form', 'verb', 'run']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5: Reconstruct Processed Text"
      ],
      "metadata": {
        "id": "zJJ7KdbRkFZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconstruct Processed Text\n",
        "processed_text_nltk = \" \".join(lemmatized_tokens)\n",
        "print(\"Processed Text (nltk):\")\n",
        "print(processed_text_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tP7C4b-WkDIf",
        "outputId": "7846fe94-eef4-4eb5-ddc1-ba96fd4e4dbf"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed Text (nltk):\n",
            "example sentence demonstrate process lemmatization run ran run different form verb run\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 6: Save the Processed Data to a File"
      ],
      "metadata": {
        "id": "i9nFDxeRkXXf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Processed Data to a File\n",
        "with open(\"processed_text_nltk.txt\", \"w\") as f:\n",
        "    f.write(processed_text_nltk)"
      ],
      "metadata": {
        "id": "2ynXIzxbkTEn"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------\n",
        "# Using spaCy for Text Processing\n",
        "# ------------------------------\n",
        "\n",
        "import spacy\n",
        "\n",
        "# Load spaCy's English language model\n",
        "# Make sure to run: python -m spacy download en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Process the text with spaCy\n",
        "doc = nlp(document)\n",
        "\n",
        "# 3 & 4. Remove stopwords & punctuation, then lemmatize using spaCy's attributes\n",
        "lemmatized_tokens_spacy = [\n",
        "    token.lemma_ for token in doc if not token.is_stop and not token.is_punct\n",
        "]\n",
        "\n",
        "# 5. Reconstruct Processed Text\n",
        "processed_text_spacy = \" \".join(lemmatized_tokens_spacy)\n",
        "print(\"\\nProcessed Text (spaCy):\")\n",
        "print(processed_text_spacy)\n",
        "\n",
        "# 6. Save the Processed Data to a File\n",
        "with open(\"processed_text_spacy.txt\", \"w\") as f:\n",
        "    f.write(processed_text_spacy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P22LWSZLkoyQ",
        "outputId": "76cd97f1-7813-4c5a-c0e1-02d7cb24d287"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Processed Text (spaCy):\n",
            "example sentence demonstrate process lemmatization running run run different form verb run\n"
          ]
        }
      ]
    }
  ]
}