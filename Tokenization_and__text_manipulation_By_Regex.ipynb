{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNWFngqzBjcc577SxlVH+H0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/raz0208/Techniques-For-Text-Analysis/blob/main/Tokenization_and__text_manipulation_By_Regex.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization and text manipulation:\n",
        "Tokenization and text manipulation are fundamental concepts in natural language processing (NLP) and data preprocessing. Let’s break them down:\n",
        "\n",
        "**- Tokenization:** It’s the process of breaking down a text into smaller units called tokens. Tokens can be words, sentences, or even subwords, depending on the task.\n",
        "  - For example: Text: \"Pedestrian detection is important.\"\n",
        "  - Word-level tokens: [\"Pedestrian\", \"detection\", \"is\", \"important\", \".\"]\n",
        "  - Character-level tokens: ['P', 'e', 'd', 'e', 's', 't', ...]\n",
        "  - Sentence-level tokens (if the text had multiple sentences) would be entire sentences.\n",
        "\n",
        "- Tokenization helps convert text into a structured format so it can be processed by machine learning models.\n",
        "\n",
        "**- Text Manipulation:** This refers to the various operations you can apply to text data to clean, transform, and prepare it. Common text manipulation techniques include:\n",
        "  - Lowercasing: Converting text to lowercase for consistency (e.g., “Detection” → “detection”).\n",
        "  - Removing punctuation: Cleaning up unnecessary characters like .,?!.\n",
        "  - Stopword removal: Removing common words like \"is\", \"the\", \"and\" that don’t carry much meaning.\n",
        "  - Stemming and Lemmatization: Reducing words to their root form (e.g., \"running\" → \"run\").\n",
        "  - Replacing or removing special characters.\n",
        "  - Normalization: Standardizing text, like converting different forms of words into one common form.\n",
        "\n",
        "Together, tokenization and text manipulation help convert raw text into a clean, structured format suitable for analysis or feeding into models."
      ],
      "metadata": {
        "id": "ekr6RaeapS2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Import libraries and read data"
      ],
      "metadata": {
        "id": "jZG-peVMqsqB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "wU-DU-ZvpSNB"
      },
      "outputs": [],
      "source": [
        "# Import required libraried\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Read Data\n",
        "text = \"Hello there!   I'm working on pedestrian   detection, it's quite exciting. 123   times more  than I expected!\"\n",
        "\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "PgDNgU5Xqzv4",
        "outputId": "d710cc8a-e1a7-44ef-aa73-d2d688e86daf"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello there!   I'm working on pedestrian   detection, it's quite exciting. 123   times more  than I expected!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Preprocess Text\n",
        "- Convert text to lowercase for consistency.\n",
        "- Remove extra spaces & special characters (optional)."
      ],
      "metadata": {
        "id": "UBs6dHtNrAXw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert text to lowercase\n",
        "text = text.lower()\n",
        "\n",
        "print(text, \"\\n\")\n",
        "\n",
        "# Remove extra spaces and special characters (optional)\n",
        "text = text.strip()\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C3Du5kZFq-lg",
        "outputId": "93129866-c743-45d6-b69e-b1919f91d7a7"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello there!   i'm working on pedestrian   detection, it's quite exciting. 123   times more  than i expected! \n",
            "\n",
            "hello there!   i'm working on pedestrian   detection, it's quite exciting. 123   times more  than i expected!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Tokenization Using Regex\n",
        "- Use regex to split words, numbers, and contractions correctly."
      ],
      "metadata": {
        "id": "PEZKmfmErmeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Regex pattern to split words, numbers, and contractions\n",
        "pattern = r\"\\b\\w+\\b|\\'\\w+|\\w+\\'\\w+\"\n",
        "tokens = re.findall(pattern, text)\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3RFTfl9Lri5g",
        "outputId": "ae4946f9-89c4-40b7-aed7-aa1443218a31"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'there', 'i', \"'m\", 'working', 'on', 'pedestrian', 'detection', 'it', \"'s\", 'quite', 'exciting', '123', 'times', 'more', 'than', 'i', 'expected']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Text Manipulation & Cleaning\n",
        "- Remove numbers (if not needed).\n",
        "- Remove punctuation (optional).\n",
        "- Expand contractions for better NLP processing."
      ],
      "metadata": {
        "id": "hNnb6eedr5Y4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove numbers (if not needed)\n",
        "tokens = [token for token in tokens if not token.isdigit()]\n",
        "\n",
        "print(tokens, \"\\n\")\n",
        "\n",
        "# Remove punctuation (optional)\n",
        "tokens = [token.translate(str.maketrans('', '', string.punctuation)) for token in tokens]\n",
        "tokens = [token for token in tokens if token]  # Remove empty strings\n",
        "\n",
        "print(tokens, \"\\n\")\n",
        "\n",
        "# Expand contractions (simple example)\n",
        "contractions = {\"i'm\": \"i am\", \"it's\": \"it is\"}\n",
        "tokens = [contractions.get(token, token) for token in tokens]\n",
        "\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vRQb_5or3eQ",
        "outputId": "4fb94d05-bf13-43ce-f30a-b09bd7c08527"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hello', 'there', 'i', \"'m\", 'working', 'on', 'pedestrian', 'detection', 'it', \"'s\", 'quite', 'exciting', 'times', 'more', 'than', 'i', 'expected'] \n",
            "\n",
            "['hello', 'there', 'i', 'm', 'working', 'on', 'pedestrian', 'detection', 'it', 's', 'quite', 'exciting', 'times', 'more', 'than', 'i', 'expected'] \n",
            "\n",
            "['hello', 'there', 'i', 'm', 'working', 'on', 'pedestrian', 'detection', 'it', 's', 'quite', 'exciting', 'times', 'more', 'than', 'i', 'expected']\n"
          ]
        }
      ]
    }
  ]
}